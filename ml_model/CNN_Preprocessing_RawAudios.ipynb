{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPftjMn9qzPmV3wLCTWHo7/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Wrapping the Mel-spectrogram extraction into the model"],"metadata":{"id":"Md5cHMfh3jrW"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yjOkUev148Xz","executionInfo":{"status":"ok","timestamp":1753881612754,"user_tz":-330,"elapsed":32775,"user":{"displayName":"Shagun Thakur","userId":"13424547828332525299"}},"outputId":"287c7761-c0c4-46a9-bafd-54e23a24dd71"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"L-XkhpuE3WYW","executionInfo":{"status":"ok","timestamp":1753881622380,"user_tz":-330,"elapsed":5883,"user":{"displayName":"Shagun Thakur","userId":"13424547828332525299"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras import layers"]},{"cell_type":"code","source":["# Constants\n","SAMPLE_RATE = 16000\n","DURATION = 2\n","N_MELS = 40\n","FRAME_LENGTH = 512\n","FRAME_STEP = 256\n","FFT_LENGTH = 512\n","EXPECTED_TIME_STEPS = 63 # The number of time steps expected by the CNN model\n","\n","# Load your trained CNN model that takes (40, 63, 1)\n","cnn_model = load_model(\"/content/drive/MyDrive/cnn_audio_classifier_approach (1).h5\")\n","\n","# ----------------------------\n","# Wrapper Model: Raw audio -> Mel Spectrogram -> CNN\n","# ----------------------------\n","\n","# Custom Keras Layer for STFT\n","class STFTLayer(layers.Layer):\n","    def __init__(self, frame_length, frame_step, fft_length, **kwargs):\n","        super(STFTLayer, self).__init__(**kwargs)\n","        self.frame_length = frame_length\n","        self.frame_step = frame_step\n","        self.fft_length = fft_length\n","\n","    def call(self, inputs):\n","        stft = tf.signal.stft(\n","            inputs,\n","            frame_length=self.frame_length,\n","            frame_step=self.frame_step,\n","            fft_length=self.fft_length\n","        )\n","        return tf.abs(stft)\n","\n","# Custom Keras Layer for Mel Spectrogram\n","class MelSpectrogramLayer(layers.Layer):\n","    def __init__(self, num_mel_bins, sample_rate, lower_edge_hertz, upper_edge_hertz, **kwargs):\n","        super(MelSpectrogramLayer, self).__init__(**kwargs)\n","        self.num_mel_bins = num_mel_bins\n","        self.sample_rate = sample_rate\n","        self.lower_edge_hertz = lower_edge_hertz\n","        self.upper_edge_hertz = upper_edge_hertz\n","\n","    def build(self, input_shape):\n","        num_spectrogram_bins = input_shape[-1]\n","        self.linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n","            num_mel_bins=self.num_mel_bins,\n","            num_spectrogram_bins=num_spectrogram_bins,\n","            sample_rate=self.sample_rate,\n","            lower_edge_hertz=self.lower_edge_hertz,\n","            upper_edge_hertz=self.upper_edge_hertz\n","        )\n","        super(MelSpectrogramLayer, self).build(input_shape)\n","\n","    def call(self, inputs):\n","        mel_spectrogram = tf.tensordot(inputs, self.linear_to_mel_weight_matrix, 1)\n","        mel_spectrogram.set_shape(inputs.shape[:-1].concatenate(self.linear_to_mel_weight_matrix.shape[-1:]))\n","        return mel_spectrogram\n","\n","# Custom Keras Layer for Log Scale\n","class LogScaleLayer(layers.Layer):\n","    def __init__(self, epsilon=1e-6, **kwargs):\n","        super(LogScaleLayer, self).__init__(**kwargs)\n","        self.epsilon = epsilon\n","\n","    def call(self, inputs):\n","        return tf.math.log(inputs + self.epsilon)\n","\n","# Custom Keras Layer for Expanding Dimensions\n","class ExpandDimsLayer(layers.Layer):\n","    def __init__(self, axis, **kwargs):\n","        super(ExpandDimsLayer, self).__init__(**kwargs)\n","        self.axis = axis\n","\n","    def call(self, inputs):\n","        return tf.expand_dims(inputs, axis=self.axis)\n","\n","# Custom Keras Layer for Cropping Time Steps\n","class CroppingTimeLayer(layers.Layer):\n","    def __init__(self, target_time_steps, **kwargs):\n","        super(CroppingTimeLayer, self).__init__(**kwargs)\n","        self.target_time_steps = target_time_steps\n","\n","    def call(self, inputs):\n","        current_time_steps = tf.shape(inputs)[1]\n","\n","        def crop():\n","            return inputs[:, :self.target_time_steps, :, :]\n","\n","        def no_change():\n","            return inputs\n","\n","        return tf.cond(tf.greater(current_time_steps, self.target_time_steps), crop, no_change)\n","\n","# Custom Keras Layer for Transposing Dimensions\n","class TransposeLayer(layers.Layer):\n","    def __init__(self, perm, **kwargs):\n","        super(TransposeLayer, self).__init__(**kwargs)\n","        self.perm = perm\n","\n","    def call(self, inputs):\n","        return tf.transpose(inputs, perm=self.perm)\n","\n","\n","# Step 1: Input layer for raw audio\n","input_audio = tf.keras.Input(shape=(SAMPLE_RATE * DURATION,), name=\"raw_audio\")\n","\n","# Step 2: Compute STFT using custom layer\n","spectrogram = STFTLayer(\n","    frame_length=FRAME_LENGTH,\n","    frame_step=FRAME_STEP,\n","    fft_length=FFT_LENGTH\n",")(input_audio)\n","\n","# Step 3: Apply Mel Filterbank using custom layer\n","mel_spectrogram = MelSpectrogramLayer(\n","    num_mel_bins=N_MELS,\n","    sample_rate=SAMPLE_RATE,\n","    lower_edge_hertz=80.0,\n","    upper_edge_hertz=7600.0\n",")(spectrogram)\n","\n","# Step 4: Convert to log scale (dB) using custom layer\n","log_mel_spectrogram = LogScaleLayer()(mel_spectrogram)\n","\n","# Step 5: Add channel dimension using custom layer\n","log_mel_spectrogram = ExpandDimsLayer(axis=-1)(log_mel_spectrogram)\n","\n","# Step 6: Crop time steps using custom layer\n","cropped_mel_spectrogram = CroppingTimeLayer(target_time_steps=EXPECTED_TIME_STEPS)(log_mel_spectrogram)\n","\n","# Step 7: Transpose dimensions to match CNN input (Batch, N_MELS, TimeSteps, Channels)\n","transposed_mel_spectrogram = TransposeLayer(perm=[0, 2, 1, 3])(cropped_mel_spectrogram)\n","\n","# Step 8: Pass through CNN model\n","output = cnn_model(transposed_mel_spectrogram)\n","\n","# Final model\n","full_model = tf.keras.Model(inputs=input_audio, outputs=output, name=\"RawAudioToPrediction\")\n","\n","# Save full model\n","full_model.save(\"cnn_with_preprocessing_2.h5\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mr-wr2H73y9s","executionInfo":{"status":"ok","timestamp":1753881921369,"user_tz":-330,"elapsed":2906,"user":{"displayName":"Shagun Thakur","userId":"13424547828332525299"}},"outputId":"15aac003-a884-4942-f41a-77770019c339"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n","WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]}]},{"cell_type":"markdown","source":["Converting the above model to .tflite"],"metadata":{"id":"WOu4KPu0-Kt5"}},{"cell_type":"code","source":[],"metadata":{"id":"H-xP2YC_ylBB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert to TFLite\n","converter = tf.lite.TFLiteConverter.from_keras_model(full_model)\n","tflite_model = converter.convert()\n","\n","# Save TFLite model\n","with open(\"cnn_with_preprocessing_2.tflite\", \"wb\") as f:\n","    f.write(tflite_model)\n","\n","print(\"âœ… Exported cnn_with_preprocessing_2.tflite\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2K9f3Oq54Sj7","executionInfo":{"status":"ok","timestamp":1753881941235,"user_tz":-330,"elapsed":3621,"user":{"displayName":"Shagun Thakur","userId":"13424547828332525299"}},"outputId":"6ce03bf5-eea9-4a42-bba2-80733ddc1f41"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved artifact at '/tmp/tmpa_vd5laq'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 32000), dtype=tf.float32, name='raw_audio')\n","Output Type:\n","  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n","Captures:\n","  140252764705744: TensorSpec(shape=(257, 40), dtype=tf.float32, name=None)\n","  140252783985488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783986832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783988944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783990288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783990096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783987984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783991824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783993552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783992016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783994128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783991632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783993360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783992592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783996624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783996816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783997392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783995856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783996048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783995088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783999696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252783999312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  140252784000656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","âœ… Exported cnn_with_preprocessing_2.tflite\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"LmObQNsm-R7j"},"execution_count":null,"outputs":[]}]}